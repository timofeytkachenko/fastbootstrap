"""Core bootstrap functionality.

This module provides the fundamental bootstrap resampling functions and
core statistical utilities for bootstrap analysis.
"""

import warnings
from typing import Callable, Iterator, Optional, Union

import numpy as np
import numpy.typing as npt
from joblib import Parallel, delayed
from scipy.stats import norm

from .constants import (
    DEFAULT_BOOTSTRAP_SAMPLES,
    DEFAULT_CONFIDENCE_LEVEL,
    DEFAULT_N_JOBS,
    DEFAULT_SEED,
    EPSILON,
    ERROR_MESSAGES,
    MIN_SAMPLE_SIZE,
)
from .exceptions import InsufficientDataError, NumericalError, ValidationError


def _validate_bootstrap_params(
    number_of_bootstrap_samples: int,
    bootstrap_conf_level: float,
    sample_size: Optional[int] = None,
) -> None:
    """Validate bootstrap parameters.

    Parameters
    ----------
    number_of_bootstrap_samples : int
        Number of bootstrap samples.
    bootstrap_conf_level : float
        Confidence level for bootstrap.
    sample_size : int, optional
        Sample size for bootstrap.

    Raises
    ------
    ValidationError
        If parameters are invalid.
    """
    if number_of_bootstrap_samples <= 0:
        raise ValidationError(
            ERROR_MESSAGES["invalid_bootstrap_samples"],
            parameter="number_of_bootstrap_samples",
            value=number_of_bootstrap_samples,
        )

    if not (0 < bootstrap_conf_level < 1):
        raise ValidationError(
            ERROR_MESSAGES["invalid_confidence_level"],
            parameter="bootstrap_conf_level",
            value=bootstrap_conf_level,
        )

    if sample_size is not None and sample_size < MIN_SAMPLE_SIZE:
        raise ValidationError(
            ERROR_MESSAGES["invalid_sample_size"],
            parameter="sample_size",
            value=sample_size,
        )


def _validate_sample_array(array: npt.NDArray[np.floating], name: str) -> None:
    """Validate a sample array.

    Parameters
    ----------
    array : ndarray
        Array to validate.
    name : str
        Name of the array for error messages.

    Raises
    ------
    ValidationError
        If array is invalid.
    InsufficientDataError
        If array is too small.
    """
    if array.size == 0:
        raise ValidationError(ERROR_MESSAGES["empty_array"], parameter=name)

    if array.size < MIN_SAMPLE_SIZE:
        raise InsufficientDataError(
            ERROR_MESSAGES["insufficient_data"],
            sample_size=array.size,
            min_required=MIN_SAMPLE_SIZE,
        )


def estimate_confidence_interval(
    distribution: npt.NDArray[np.floating],
    bootstrap_conf_level: float = DEFAULT_CONFIDENCE_LEVEL,
) -> npt.NDArray[np.floating]:
    """Estimate confidence interval from a bootstrap distribution.

    Parameters
    ----------
    distribution : ndarray
        1D array containing the bootstrap distribution.
    bootstrap_conf_level : float, optional
        Confidence level between 0 and 1. Default is 0.95.

    Returns
    -------
    ndarray
        Array [lower_bound, upper_bound] representing the confidence interval.

    Raises
    ------
    ValidationError
        If distribution is empty or confidence level is invalid.
    NumericalError
        If quantile computation fails.

    Examples
    --------
    >>> distribution = np.array([1, 2, 3, 4, 5])
    >>> ci = estimate_confidence_interval(distribution, 0.95)
    >>> len(ci)
    2

    Notes
    -----
    Time complexity: O(n log n) where n is the distribution length.
    Space complexity: O(1).
    """
    _validate_sample_array(distribution, "distribution")
    _validate_bootstrap_params(1, bootstrap_conf_level)

    try:
        left_quant = (1 - bootstrap_conf_level) / 2
        right_quant = 1 - left_quant
        return np.quantile(distribution, [left_quant, right_quant])
    except (ValueError, TypeError) as e:
        raise NumericalError(
            "Failed to compute confidence interval",
            operation="quantile_computation",
            values={"confidence_level": bootstrap_conf_level},
        ) from e


def estimate_p_value(
    bootstrap_difference_distribution: npt.NDArray[np.floating],
    number_of_bootstrap_samples: int,
) -> float:
    """Estimate two-sided p-value from bootstrap difference distribution.

    Parameters
    ----------
    bootstrap_difference_distribution : ndarray
        1D array containing the bootstrap difference distribution.
    number_of_bootstrap_samples : int
        Number of bootstrap samples used to generate the distribution.

    Returns
    -------
    float
        The estimated two-sided p-value.

    Raises
    ------
    ValidationError
        If distribution is empty or number of samples is invalid.
    NumericalError
        If p-value computation fails.

    Examples
    --------
    >>> distribution = np.array([-1, 0, 1, 2, 3])
    >>> p_value = estimate_p_value(distribution, 5)
    >>> 0 <= p_value <= 1
    True

    Notes
    -----
    Time complexity: O(n) where n is the distribution length.
    Space complexity: O(1).
    """
    _validate_sample_array(
        bootstrap_difference_distribution, "bootstrap_difference_distribution"
    )
    _validate_bootstrap_params(number_of_bootstrap_samples, 0.95)

    try:
        positions = np.sum(bootstrap_difference_distribution < 0, axis=0)
        return float(
            2
            * np.minimum(positions, number_of_bootstrap_samples - positions)
            / number_of_bootstrap_samples
        )
    except (ValueError, TypeError) as e:
        raise NumericalError(
            "Failed to compute p-value",
            operation="p_value_computation",
            values={"sample_count": number_of_bootstrap_samples},
        ) from e


def estimate_bin_params(sample: npt.NDArray[np.floating]) -> tuple[float, int]:
    """Estimate optimal histogram bin parameters using Freedman-Diaconis rule.

    Parameters
    ----------
    sample : ndarray
        1D array containing observations.

    Returns
    -------
    tuple[float, int]
        A tuple (bin_width, bin_count) where:
        - bin_width : float
            Width of each bin using Freedman-Diaconis rule.
        - bin_count : int
            Number of bins for the histogram.

    Raises
    ------
    ValidationError
        If sample is empty.
    NumericalError
        If bin computation fails.

    Examples
    --------
    >>> sample = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    >>> bin_width, bin_count = estimate_bin_params(sample)
    >>> bin_width > 0 and bin_count > 0
    True

    Notes
    -----
    Time complexity: O(n log n) where n is the sample length.
    Space complexity: O(1).
    """
    _validate_sample_array(sample, "sample")

    try:
        q1 = np.quantile(sample, 0.25)
        q3 = np.quantile(sample, 0.75)
        iqr = q3 - q1

        # Freedman-Diaconis rule
        bin_width = (2 * iqr) / (sample.shape[0] ** (1 / 3))

        # Ensure bin_width is not zero
        if bin_width < EPSILON:
            bin_width = (sample.max() - sample.min()) / 10

        bin_count = max(1, int(np.ceil((sample.max() - sample.min()) / bin_width)))

        return float(bin_width), bin_count
    except (ValueError, TypeError) as e:
        raise NumericalError(
            "Failed to compute bin parameters",
            operation="bin_computation",
            values={"sample_size": sample.size},
        ) from e


def jackknife_indices(
    sample: npt.NDArray[np.floating],
) -> Iterator[npt.NDArray[np.intp]]:
    """Generate jackknife indices for leave-one-out resampling.

    Parameters
    ----------
    sample : ndarray
        1D array containing the sample data.

    Yields
    ------
    ndarray
        Arrays of indices with one element removed for jackknife resampling.

    Raises
    ------
    ValidationError
        If sample is empty.

    Examples
    --------
    >>> sample = np.array([1, 2, 3, 4, 5])
    >>> indices_list = list(jackknife_indices(sample))
    >>> len(indices_list) == len(sample)
    True

    Notes
    -----
    Time complexity: O(n) per yielded array where n is the sample length.
    Space complexity: O(n) per yielded array.
    """
    _validate_sample_array(sample, "sample")

    base_indices = np.arange(len(sample))
    for i in range(len(sample)):
        yield np.delete(base_indices, i)


def bca_confidence_interval(
    sample: npt.NDArray[np.floating],
    bootstrap_distribution: npt.NDArray[np.floating],
    statistic: Callable[[npt.NDArray[np.floating]], float] = np.mean,
    bootstrap_conf_level: float = DEFAULT_CONFIDENCE_LEVEL,
) -> npt.NDArray[np.floating]:
    """Compute BCa (bias-corrected and accelerated) confidence interval.

    Parameters
    ----------
    sample : ndarray
        1D array containing the original sample data.
    bootstrap_distribution : ndarray
        1D array containing the bootstrap distribution of the statistic.
    statistic : callable, optional
        Function that computes the statistic of interest.
        Default is np.mean.
    bootstrap_conf_level : float, optional
        Confidence level between 0 and 1. Default is 0.95.

    Returns
    -------
    ndarray
        Array [lower_bound, upper_bound] representing the BCa confidence interval.

    Raises
    ------
    ValidationError
        If inputs are invalid.
    NumericalError
        If BCa computation fails.

    Examples
    --------
    >>> sample = np.array([1, 2, 3, 4, 5])
    >>> bootstrap_dist = np.array([2.5, 3.0, 2.8, 3.2, 2.9])
    >>> ci = bca_confidence_interval(sample, bootstrap_dist)
    >>> len(ci)
    2

    Notes
    -----
    Time complexity: O(nÂ² + b log b) where n is sample size, b is bootstrap samples.
    Space complexity: O(n + b).
    """
    _validate_sample_array(sample, "sample")
    _validate_sample_array(bootstrap_distribution, "bootstrap_distribution")
    _validate_bootstrap_params(len(bootstrap_distribution), bootstrap_conf_level)

    try:
        number_of_bootstrap_samples = bootstrap_distribution.shape[0]
        sample_stat = statistic(sample)

        # Confidence interval alphas
        alphas = np.array(
            [(1 - bootstrap_conf_level) / 2, 1 - (1 - bootstrap_conf_level) / 2]
        )

        # Bias correction value
        z0 = norm.ppf(
            np.sum(bootstrap_distribution < sample_stat) / number_of_bootstrap_samples
        )

        # Compute jackknife statistics for acceleration
        jackknife_stats = [
            statistic(sample[indices]) for indices in jackknife_indices(sample)
        ]
        jack_mean = np.mean(jackknife_stats)

        # Acceleration value
        numerator = np.sum((jack_mean - jackknife_stats) ** 3)
        denominator = 6.0 * (np.sum((jack_mean - jackknife_stats) ** 2) ** 1.5)

        if abs(denominator) < EPSILON:
            acceleration = 0.0
            warnings.warn(
                "Acceleration value undefined due to zero denominator. "
                "Using percentile method instead.",
                UserWarning,
                stacklevel=2,
            )
        else:
            acceleration = numerator / denominator

        # Compute BCa endpoints
        z_alphas = z0 + norm.ppf(alphas)
        adjusted_alphas = norm.cdf(z0 + z_alphas / (1 - acceleration * z_alphas))

        indices = np.round((number_of_bootstrap_samples - 1) * adjusted_alphas).astype(
            int
        )
        indices = np.clip(indices, 0, number_of_bootstrap_samples - 1)

        sorted_distribution = np.sort(bootstrap_distribution)
        return sorted_distribution[indices]

    except (ValueError, TypeError, ZeroDivisionError) as e:
        raise NumericalError(
            "Failed to compute BCa confidence interval",
            operation="bca_computation",
            values={
                "sample_size": sample.size,
                "bootstrap_samples": len(bootstrap_distribution),
            },
        ) from e


def _create_rng_from_seed(seed_sequence: np.random.SeedSequence) -> np.random.Generator:
    """Create a random number generator from a seed sequence.
    
    Helper function to enable lazy RNG creation in parallel processing,
    reducing memory overhead by avoiding upfront instantiation of all RNGs.
    
    Parameters
    ----------
    seed_sequence : np.random.SeedSequence
        Seed sequence for RNG initialization.
    
    Returns
    -------
    np.random.Generator
        Initialized random number generator.
        
    Notes
    -----
    Time complexity: O(1).
    Space complexity: O(1).
    """
    return np.random.Generator(np.random.PCG64(seed_sequence))


def _execute_bootstrap_sample(
    sample_function: Callable[[np.random.Generator], Union[float, npt.NDArray[np.floating]]],
    seed_sequence: np.random.SeedSequence,
) -> Union[float, npt.NDArray[np.floating]]:
    """Execute a single bootstrap sample with lazy RNG creation.
    
    Combines RNG creation and sampling to minimize memory footprint
    by creating generators only when needed in each parallel worker.
    
    Parameters
    ----------
    sample_function : callable
        Function that computes bootstrap statistic.
    seed_sequence : np.random.SeedSequence
        Seed for reproducible RNG initialization.
    
    Returns
    -------
    float or ndarray
        Bootstrap statistic result.
        
    Notes
    -----
    Time complexity: O(f) where f is sample_function cost.
    Space complexity: O(1) + O(r) where r is result size.
    """
    rng = _create_rng_from_seed(seed_sequence)
    return sample_function(rng)


def bootstrap_resampling(
    sample_function: Callable[
        [np.random.Generator], Union[float, npt.NDArray[np.floating]]
    ],
    number_of_bootstrap_samples: int = DEFAULT_BOOTSTRAP_SAMPLES,
    seed: Optional[int] = DEFAULT_SEED,
    n_jobs: int = DEFAULT_N_JOBS,
    batch_size: Optional[int] = None,
) -> npt.NDArray[np.floating]:
    """Perform bootstrap resampling with optimized parallel processing.
    
    Efficiently generates bootstrap statistics using lazy RNG creation,
    batch processing, and optimized memory management for large-scale datasets.

    Parameters
    ----------
    sample_function : callable
        Function that takes a NumPy Generator and returns a bootstrap statistic.
        Signature: sample_function(generator: np.random.Generator) -> float or ndarray
    number_of_bootstrap_samples : int, optional
        Number of bootstrap samples to generate. Default is 10000.
    seed : int, optional
        Seed for reproducibility. Default is 42.
    n_jobs : int, optional
        Number of parallel jobs. -1 uses all available cores. Default is -1.
    batch_size : int, optional
        Number of samples per batch for parallel processing. 
        If None, uses 'auto' for dynamic batch sizing. 
        Larger batches reduce overhead but increase memory per worker.
        Default is None (auto).

    Returns
    -------
    ndarray
        Array of bootstrap statistics with shape (number_of_bootstrap_samples,).

    Raises
    ------
    ValidationError
        If parameters are invalid.
    NumericalError
        If bootstrap computation fails.

    Examples
    --------
    >>> def sample_mean(rng):
    ...     return rng.normal(0, 1, 100).mean()
    >>> results = bootstrap_resampling(sample_mean, 1000)
    >>> len(results)
    1000
    
    >>> # For large datasets, specify batch size for optimal performance
    >>> results = bootstrap_resampling(sample_mean, 1000000, batch_size=1000)
    >>> len(results)
    1000000

    Notes
    -----
    **Optimizations:**
    
    1. **Speed**: Uses lazy RNG generation and batch processing to minimize 
       overhead. Suitable for datasets with >1M entries.
    2. **Memory**: Avoids upfront RNG list creation, reducing memory by ~O(n).
       Creates generators on-demand in parallel workers.
    3. **Parallelism**: Uses joblib with optimized batch_size and 'processes' 
       backend for CPU-bound bootstrap operations.
    4. **Readability**: Modular design with helper functions for clear separation
       of concerns.
    
    Time complexity: O(n * f) where n is bootstrap samples, f is sample function cost.
    Space complexity: O(n) for results only, avoiding intermediate storage.
    """
    _validate_bootstrap_params(number_of_bootstrap_samples, 0.95)

    try:
        # Generate seed sequences lazily to avoid storing all RNGs in memory
        base_seed_sequence = np.random.SeedSequence(seed)
        spawned_seeds = base_seed_sequence.spawn(number_of_bootstrap_samples)
        
        # Configure parallel execution with optimized settings
        # - prefer='processes': Better for CPU-bound bootstrap operations
        # - batch_size: Controls memory/speed tradeoff
        # - return_as='generator': Could be used for streaming, but array is more efficient here
        parallel_executor = Parallel(
            n_jobs=n_jobs,
            prefer='processes',
            batch_size=batch_size or 'auto',
        )
        
        # Execute bootstrap samples in parallel with lazy RNG creation
        # Seeds are passed directly, RNGs created in workers
        bootstrap_results = parallel_executor(
            delayed(_execute_bootstrap_sample)(sample_function, seed_seq)
            for seed_seq in spawned_seeds
        )
        
        # Convert to numpy array efficiently (joblib returns list)
        return np.asarray(bootstrap_results, dtype=np.float64)

    except (ValueError, TypeError) as e:
        raise NumericalError(
            "Failed to perform bootstrap resampling",
            operation="bootstrap_resampling",
            values={"n_samples": number_of_bootstrap_samples, "n_jobs": n_jobs},
        ) from e
